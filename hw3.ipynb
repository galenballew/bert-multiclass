{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Intent Classifier with Transformers\n",
    "\n",
    "In assignment 2, you have built a simple classifier with traditional machine learning methods. In this assignment, you are going to have a hands-on experience of newer and larger pre-trained models, particularly in Transformers, which is an architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. Transformers compute representations of its input and output using its self-attention mechanism. For further reading about Transformers, please refer to [this well-written blog](https://nlp.seas.harvard.edu/2018/04/03/attention.html) or the [original paper](https://arxiv.org/abs/1706.03762). Secondly, you are going create a similar intent data set of your own based on UW CSE course catalogs.\n",
    "\n",
    "This assignment will mainly focus helping you getting familiar with [Pytorch](https://pytorch.org/), an open source machine learning library based on the `Torch` library, and ['transformers' library from Huggingface](https://huggingface.co/transformers/), as well as learning to create good-quality datasets. It is okay if you would like to continue with `Tensorflow`, as long as you have your write-up questions correctly arranged.\n",
    "\n",
    "Before you start writing any code, please read through this specification, understand the questions.\n",
    "\n",
    "## Setting up your environment\n",
    "\n",
    "This assignment will be presented in [Jupyter Notebook](https://ipython.org/notebook.html), making it easier for students without GPUs to utilize resources from [Google Colab](https://colab.research.google.com/), [DeepNote](https://deepnote.com/) or other platform that provide free access to GPU/devices. However, if you prefer to not use Jupyter Notebook, please be sure to include your write up file in `hw2_writeup.pdf` in your repository.\n",
    "\n",
    "### Installing Dependencies\n",
    "\n",
    "Following the dependencies you installed in assignment 2, you should also install `Pytorch` from [this page](https://pytorch.org/), be sure to select the correct OS, package, and compute platform. If you are using newer GPUs such as RTX3090, you need to install a specific version of `Pytorch` and `CUDA`, for which [this page](https://lambdalabs.com/blog/install-tensorflow-and-pytorch-on-rtx-30-series/) may be helpful.\n",
    "\n",
    "Then, you can install `transformers` library with `pip` or `conda`.\n",
    "\n",
    "### Using Colab/DeepNote\n",
    "\n",
    "To use Colab or DeepNote, you can simply upload this notebook as well as your data/files and run it with Google/DeepNote's computing resources. When you are done with this notebook, simply click `File`->`Download` and save it to your repository. Be sure to select GPU, otherwise, it may takes hours to run on CPU.\n",
    "\n",
    "For further instructions, see [this blog for `Colab`](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) and [this video](deepnote.com) for DeepNote.\n",
    "\n",
    "## Tips\n",
    "\n",
    "* Be sure to *select GPU* for your Colab/DeepNote(`Edit`->`Notebook Settings`), otherwise it will take hours for you to run the code.\n",
    "* If you are using `Windows WSL` with `CUDA` and find it very slow, consider not using `WSL`, whose support for `CUDA` was limited(in addition, only `WSL2` support `CUDA`).\n",
    "* You may find documentation for `transformers` particularly useful for this assignment.\n",
    "* You may want to be careful on creating UW CSE course catalog datasets, because it is going to be used in the following assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "# Import transformers library here\n",
    "# TODO: If you would like to use models other than DistilBert, you can change the import here and names in later part\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data preprocessing & Tokenization\n",
    "\n",
    "In this section, you are going to\n",
    "\n",
    "* Read in you train, validation, and test data.\n",
    "* Convert the categories of data into ids.\n",
    "* Tokenize your texts.\n",
    "* Create a IntentDataset class for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Read data, replace the dummy file names with your file paths\n",
    "train_texts, train_labels = tools.read_data(\"train\")\n",
    "val_texts, val_labels = tools.read_data(\"val\")\n",
    "test_texts, test_labels = tools.read_data(\"test\")\n",
    "train_texts = train_texts.tolist()\n",
    "val_texts = val_texts.tolist()\n",
    "test_texts = test_texts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create integer class labels instead of strings\n",
    "classes = tools.labels(train_labels).tolist()\n",
    "train_labels = tools.relabel(train_labels, classes)\n",
    "val_labels = tools.relabel(val_labels, classes)\n",
    "test_labels = tools.relabel(test_labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You will need a tokenier to translate human readable text to machine readable representations\n",
    "# TODO: Initialize a tokenizer that convert your texts into encodings\n",
    "\n",
    "# TODO: Use the tokenizer to get representations of your texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IntentDataset\n",
    "\n",
    "Making your data prepration easier and more extendable will save much effort. The Dataset class provided by `PyTorch` is one of the tools that make data loading simpler.\n",
    "\n",
    "In this part, you are going to create a [PyTorch Dataset class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for the data. Your `IntentDataset` should inherit `Dataset` and override below methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        To support the indexing such that dataset[i] can be used to get the i-th sample\n",
    "        \"\"\"\n",
    "        # TODO: Implement this\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the dataset.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this\n",
    "\n",
    "        return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Turn the encodings and labels to a dataset object\n",
    "train_dataset = IntentDataset(train_encodings, train_labels)\n",
    "val_dataset = IntentDataset(val_encodings, val_labels)\n",
    "test_dataset = IntentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Initialize model and Train/Validate function\n",
    "\n",
    "In this part, you are going to \n",
    "* initialize a classification model from `transformers`.\n",
    "* Implement the train function.\n",
    "* Implement the validate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize model from pretrained. Be sure that your model should be consistent with your tokenizer and you have correct num_labels field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model with an epoch\n",
    "    Arguments:\n",
    "        dataloader : the pytorch dataloader that parse data into tensors\n",
    "        optimizer : to optimize the loss\n",
    "        device : current device\n",
    "\n",
    "    Returns:\n",
    "        a list of predicted labels, a list of true labels, average loss\n",
    "    \"\"\"\n",
    "    # Initialize the labels and total loss\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    tot_loss = 0.0\n",
    "\n",
    "    # TODO: Implement this, following the comments as hint(Each comment can be done within a line of code)\n",
    "    # Let model enter training mode\n",
    "\n",
    "    # Loop through each batch to process\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # Save original labels for evaluation\n",
    "        \n",
    "        # Set device for everything in batch\n",
    "        \n",
    "        # Clear previously calculated gradients\n",
    "        \n",
    "        # Feed items in batch into the forward pass\n",
    "        \n",
    "        # Record the loss and logits\n",
    "        \n",
    "        # Backward pass with loss\n",
    "        \n",
    "        # Add loss value into tot_loss\n",
    "        \n",
    "        # Proceed a step with optimizer\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        \n",
    "        # Convert these logits to list of predicted labels values.\n",
    "        \n",
    "        \n",
    "    return pred_labels, true_labels, tot_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, device):\n",
    "    \"\"\"\n",
    "    Validate the model with an epoch\n",
    "    Arguments:\n",
    "        dataloader : the pytorch dataloader that parse data into tensors\n",
    "        device : current device\n",
    "\n",
    "    Returns:\n",
    "        a list of predicted labels, a list of true labels, average loss\n",
    "    \"\"\"\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    tot_loss = 0.0\n",
    "\n",
    "    # TODO: Implement this, following the comments as hint. validate()\n",
    "    # is going to be very simiar to train()\n",
    "\n",
    "    # Let model enter evaluation mode\n",
    "    \n",
    "    # Loop through each batch to process\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # Save original labels for evaluation\n",
    "        \n",
    "        # Set device for everything in batch\n",
    "        \n",
    "        # Not compute gradients to save memory and speedup\n",
    "        with torch.no_grad():\n",
    "            # Feed items in batch into the forward pass\n",
    "            \n",
    "            # Record the loss and logits\n",
    "            \n",
    "            # Add loss value into tot_loss\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            \n",
    "            # Convert these logits to list of predicted labels values.\n",
    "            \n",
    "\n",
    "    return pred_labels, true_labels, tot_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Fine-tune your model\n",
    "\n",
    "Good work! You have now completed the core parts of fine-tuning model. \n",
    "In this section, will be implementing the final parts of the fine-tuning process. \n",
    "Train and validate your model recording the accuracy and loss along the way.\n",
    "It may take some time to run this section, so please be patient and double check for typos before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Create optimizer, set your learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# Set device variable\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Create dataloader for train and validation dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "# Record losses and accuricies for each epoch\n",
    "losses = {'train_loss':[], 'val_loss':[]}\n",
    "accuracies = {'train_acc':[], 'val_acc':[]}\n",
    "# Define the number of epoches you will want to run\n",
    "epoch_num = 3\n",
    "\n",
    "model.to(device)\n",
    "# TODO: Loop through each epoch, train and validate\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    # TODO: Implement this. Each line has comment and beginning as hint.\n",
    "    print(\"Processing epoch \", epoch)\n",
    "    # Fine-tune using the training set\n",
    "    train_pred, train_labels, train_loss = \n",
    "    # Get validation results\n",
    "    val_pred, val_labels, val_loss = \n",
    "    # Compute the accuracies using the results\n",
    "    train_acc = \n",
    "    val_acc = \n",
    "    # Record the accuracies and losses for train and validation\n",
    "    accuracies['train_acc'].append(train_acc)\n",
    "    losses['train_loss'].append(train_loss)\n",
    "    accuracies['val_acc'].append(val_acc)\n",
    "    losses['val_loss'].append(val_loss)\n",
    "\n",
    "print(losses)\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Evaluation and Analysis\n",
    "In this section, you are going to\n",
    "* Plot the training and validation loss/accuracy with respect to epochs you ran.\n",
    "* Compute the performance metrics (precision, F1, recall) of your _final model_.\n",
    "* Compare these results with your model from Assignment 2.\n",
    "\n",
    "No extra code is required in this section, but you should run it and observe the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss with respect to epoches\n",
    "plt.plot(losses['train_loss'], 'r--', label='train loss')\n",
    "plt.plot(losses['val_loss'], 'b', label='validation loss')\n",
    "plt.title(\"Loss wrt Epoch\")\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks([0, 1, 2], [1, 2, 3])\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuricies with respect to epoches\n",
    "plt.plot(accuracies['train_acc'], 'r--', label='train accuracy')\n",
    "plt.plot(accuracies['val_acc'], 'b', label='validation accuracy')\n",
    "plt.title(\"Accuracy wrt Epoch\")\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.xticks([0, 1, 2], [1, 2, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using the test set at the very end\n",
    "# Note: You should only run this cell ONCE at the very end after you have completed any tuning\n",
    "#       and training. \n",
    "#       It is inadvisible to develop your model against the test set as you can end up\n",
    "#       inadvertantly overfitting to the test data. \n",
    "from sklearn.metrics import classification_report\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "pred_labels, true_labels, _ = validate(test_loader, device)\n",
    "# Compute evaluate report\n",
    "report = classification_report(true_labels, pred_labels, labels=[i for i in range(len(classes))], target_names=classes)\n",
    "print()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Answer the Following Questions\n",
    "\n",
    "For each the questions below, either write a short paragraph or report the metrics asked with clear annotation.\n",
    "\n",
    "#### 1. What model and optimizer did you tried?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "#### 2. How long did it take for you to fine-tune your model? How does it compare to assignment 2?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "#### 3. Report your general accuracy for train, validation, and test set here.\n",
    "\n",
    "YOUR ANSWRE HERE\n",
    "\n",
    "#### 4. How was the performance compare to assignment 2? Why is it the case?\n",
    "\n",
    "YOUR ANSWRE HERE\n",
    "\n",
    "#### 5. Did you observe any trends from the plot of loss/accuracy with respect to epochs?\n",
    "\n",
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Create your own dataset\n",
    "\n",
    "Look at the page of [UW CSE course catalog](https://www.cs.washington.edu/education/courses/) and review the intent dataset you have worked with. \n",
    "You would like to support 3 new intents called \"cse_course_content\", \"cse_course_prerequisites\" and \"cse_course_id\" which models a student asking questions about various aspects (what content a certain course covers, what prequisites a certain course has, and what course ids cover a certain type of content) of the course offerings. \n",
    "Can you come up with some questions that a student may ask a virtual assistant for these intents? Valid questions should be answerable by a human with only the information on the course catalog page.\n",
    "\n",
    "For example, a question about \"cse_course_content\" might be something like: 'What is CSE P 590B about?'\n",
    "\n",
    "For this part, your goal is to create some training data for these new intents that you would like to support. Brainstorm at least 10 questions for _each_ intent, following the same format as the intent dataset you were working with. Create a new file `data/my_intents_train.json`.\n",
    "Tips: Refer to the existing examples in the dataset provided for inspiration on how to come up with training examples. Remember, because the training data is concrete, even small variations like a different course id can constitute a separate question. For grading purposes we're OK with even small variations, though it's still a good idea to make some effort to come up with as many diverse phrasings as you can (like in the provided dataset) as this new data will be useful in future assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "For submission, along with other files provided by this assignment you should include a report `hw3.pdf` which you can export from\n",
    "\n",
    "the notebook. This report which should include a history of your fine-tuning process, classification report and any plots that were generated. \n",
    "\n",
    "You can also use other means to create the report if you're not using a notebook as long as it has all the information included.\n",
    "\n",
    "You should also make sure to include the training data you created in `data/my_intents_train.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Implement a Model with A Custom Architecture or Train With Your New Intents\n",
    "\n",
    "In this assignment, you created a classifier with `transformers` provided by `Huggingface`. However, you can also build everything from scratch and define your own architecture. For an option of extra credit, you can implement a simple alternative neural model such as a Recurrent Neural Network (RNN), or try some different layer setup. Include the code for your model and report the accuracy (train, dev, test) for the model you trained. The procedure will be very similar to the previous parts of this assignment, except this time you will be designing your own forward pass.\n",
    "\n",
    "If you decide to do this part, [this classification tutorial provided by Pytorch](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) may be helpful.\n",
    "\n",
    "Another option of extra credit is to train with your new intents, you can either fine-tune some model with your new intent dataset or train some traditional models as you did in assignment 2. Include the code for your model and report the accuracy (train, dev, test) for the model you trained/fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report your test accuracy values here. (Optional)\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "56967ed928c518518e6427ba56498060ad9844c96f8d961079d46bd6e5ce38c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
